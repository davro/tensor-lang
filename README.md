# TensorLang: A Native AI/ML Language (Proof of Concept)

TensorLang is an open-source, native machine learning (ML) language designed to compile tensor operations directly into GPU-accelerated kernels. This proof-of-concept (PoC) addresses pain points in Python-based ML frameworks by offering a streamlined, hardware-aware alternative.

---
## Table of Contents
- [Overview](#overview)
- [Features](#features)
- [Setup](#setup)
  - [System Requirements](#system-requirements)
  - [Clone](#clone)
  - [Build](#build)
- [Tests](#tests)
- [Examples](#examples)
- [Cache](#cache)
- [Code Examples](#code-examples)
- [Future Work](#future-work)
- [Contributing](#contributing)
- [License](#license)

---

## Overview
TensorLang aims to eliminate Python bottlenecks, leverage GPU acceleration natively, and provide a unified stack for parsing, type checking, and code generation. This PoC demonstrates:
- Parsing tensor declarations and operations.
- Building an Abstract Syntax Tree (AST).
- Type checking for tensor shapes and data types.
- Generating and executing CUDA kernels via PyCUDA.
- Supporting core operations like `matmul`, `add`, `relu`, and `fill`.

## Features
- **Tensor Declarations**: Define tensors with explicit types (e.g., `Tensor[f32, (2, 3)]`).
- **Core Operations**: Implements `matmul`, `add`, `relu`, and `fill` with CUDA execution.
- **GPU Acceleration**: Compiles to CUDA kernels for NVIDIA GPUs using PyCUDA.
- **Verified Results**: Passes 8 test cases, including a stress test with 4096x4096 matrices.
- **Comment Support**: Handles single-line (`//`) and multi-line (`/* */`) comments.

## Setup
Clone the repository and prepare the environment to build and run TensorLang, follow these steps:

### System Requirements
```bash
sudo apt install nvidia-cuda-toolkit
sudo apt install python3.12-venv
```

### Clone
```bash
# TensorLang clone the repository.
git clone https://github.com/davro/tensor-lang.git
cd tensor-lang
```

### Build
```bash
# Ensure the build script is executable to activate the venv environment.
chmod +x build.sh

# This causes your current shell to execute the commands in the venv environment.
source build.sh
```

## Tests

### Parser/Compiler run single test
```bash
python3 tensorlang.py tests/add.tl
```

### Test Runner run all tests
```bash
python3 tests/runner.py
```

## Cache 
TensorLang stores all compilation outputs, intermediate files, and results in a cache directory to facilitate debugging and reusability. The cache is organized by the input file name, ensuring each TensorLang program has its own dedicated subdirectory. For example, running python3 tensorlang.py tests/add.tl creates a directory cache/tests_add.tl/.

### Cache Structure
The cache directory for a given input file (e.g., tests/add.tl) contains the following files:
 * kernel.cu: The generated CUDA source code containing kernels for operations like fill, add, mult, div, matmul, and relu. This file is automatically generated and should not be edited manually.
* kernel.so: The compiled CUDA shared object file, used to execute the kernels on the GPU.
* <tensor_name>.npy: NumPy array files storing the output tensors for each variable defined in the program (e.g., a.npy, b.npy, c.npy for tensors a, b, and c).
* Logs: Console output, including the parsed AST, type checker environment, and execution results, is printed during runtime. These logs are not saved to a file by default but can be redirected (e.g., python3 tensorlang.py tests/add.tl > log.txt).



## Examples
The examples use compiled cuda kernel from the cache


## Future Work
 * Add ReLU and other operations.
 * Support dynamic shape inference.
 * Optimize CUDA kernels with shared memory or cuBLAS.
 * Integrate MLIR/XLA for a unified compiler stack.

## Contributing

Contributions are Welcome! See [CONTRIBUTING.md] for guidelines.

## License
Licensed under [LGPL v3/MIT] (see LICENSE).